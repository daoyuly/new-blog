---
title: Agentic Design Patterns - Chapter 9_ Learning and Adaptation
tags: AI agent 设计
abbrlink: 22930
date: 2025-10-14 08:05:02
---

# 第 9 章：学习和适应

学习和适应对于增强人工智能 Agent 的能力至关重要。这些过程使 Agent 能够超越预定义参数进行演化，通过经验和环境交互实现自主改进。通过学习和适应，Agent 可以有效应对新情况并优化其性能，而无需持续的人工干预。本章将深入探讨支撑 Agent 学习和适应的原理与机制。

## 全局视角

Agent 通过基于新经验和数据改变其思维、行动或知识来实现学习和适应。这使得 Agent 能够从简单地遵循指令演化为随时间推移变得更加智能。

* **强化学习：** Agent 尝试不同的行动，对积极结果获得奖励，对消极结果受到惩罚，从而在动态环境中学习最优行为。适用于控制机器人或玩游戏的 Agent。
* **监督学习：** Agent 从标注示例中学习，建立输入与期望输出之间的映射关系，实现决策制定和模式识别等任务。适用于分类电子邮件或预测趋势的 Agent。
* **无监督学习：** Agent 在未标注数据中发现隐藏的连接和模式，有助于获得洞察、进行组织并构建其环境的心理地图。适用于在没有特定指导的情况下探索数据的 Agent。
* **基于 LLM 的 Agent 的少样本/零样本学习：** 利用大语言模型的 Agent 能够用最少的示例或清晰的指令快速适应新任务，实现对新命令或情况的快速响应。
* **在线学习：** Agent 持续使用新数据更新知识，对于动态环境中的实时响应和持续适应至关重要。适用于处理连续数据流的 Agent。
* **基于内存的学习：** Agent 回忆过去的经验以在类似情况下调整当前行动，增强上下文感知和决策能力。对于具备记忆召回能力的 Agent 特别有效。

Agent 基于学习结果改变策略、理解或目标来实现适应。这对于在不可预测、变化或新环境中的 Agent 至关重要。

**近端策略优化（PPO）** 是一种强化学习算法，用于在具有连续动作范围的环境中训练 Agent，例如控制机器人的关节或游戏中的角色。其主要目标是可靠且稳定地改进 Agent 的决策策略（即其策略）。

PPO 的核心思想是对 Agent 的策略进行小幅而谨慎的更新，避免可能导致性能崩溃的剧烈变化。其工作原理如下：

1. **收集数据：** Agent 使用其当前策略与环境交互（例如，玩游戏）并收集一批经验数据（状态、动作、奖励）。
2. **评估代理目标：** PPO 计算潜在策略更新将如何改变预期奖励。然而，它不仅仅是最大化这个奖励，而是使用特殊的"裁剪"目标函数。
3. **裁剪机制：** 这是 PPO 稳定性的关键。它在当前策略周围创建一个"信任区域"或安全区，阻止算法进行与当前策略差异过大的更新。这种裁剪机制就像一个安全刹车，确保 Agent 不会采取巨大而有风险的步骤来破坏其学习成果。

简而言之，PPO 在改进性能与保持接近已知有效策略之间取得平衡，这可以防止训练期间的灾难性故障并实现更稳定的学习过程。

**直接偏好优化（DPO）** 是一种专门为使大语言模型与人类偏好保持一致而设计的更新方法。它为此任务提供了比使用 PPO 更简单、更直接的替代方案。

要理解 DPO，首先了解传统的基于 PPO 的对齐方法会有所帮助：

* **PPO 方法（两步过程）：**
  1. **训练奖励模型：** 首先收集人类反馈数据，人们在其中评级或比较不同的 LLM 响应（例如，"响应 A 比响应 B 更好"）。这些数据用于训练一个独立的 AI 模型，称为奖励模型，其任务是预测人类会给任何新响应打什么分数。
  2. **使用 PPO 微调：** 接下来使用 PPO 微调 LLM。LLM 的目标是生成能够从奖励模型获得最高分的响应。奖励模型在训练过程中充当"评判员"。

这个两步过程可能既复杂又不稳定。例如，LLM 可能会找到漏洞并学会"破解"奖励模型，为质量较差的响应获得高分。

* **DPO 方法（直接过程）：** DPO 完全跳过了奖励模型。它不是将人类偏好转换为奖励分数然后优化该分数，而是直接使用偏好数据来更新 LLM 的策略。
* 它通过利用直接将偏好数据与最优策略联系起来的数学关系来工作。本质上，它教导模型："增加生成类似*偏好*响应的概率，减少生成类似*不受欢迎*响应的概率。"

本质上，DPO 通过直接在人类偏好数据上优化语言模型来简化对齐过程。这避免了训练和使用单独奖励模型的复杂性和潜在不稳定性，使对齐过程更加高效和稳健。

## 实际应用与用例

自适应 Agent 通过由经验数据驱动的迭代更新，在可变环境中表现出增强的性能。

* **个性化助手 Agent：** 通过对个人用户行为的纵向分析来改进交互协议，确保高度优化的响应生成。
* **交易机器人 Agent：** 通过基于高分辨率、实时市场数据动态调整模型参数来优化决策算法，从而最大化财务回报并降低风险因素。
* **应用程序 Agent：** 通过基于观察到的用户行为进行动态修改来优化用户界面和功能，从而提升用户参与度和系统直观性。
* **机器人和自动驾驶车辆 Agent：** 通过整合传感器数据和历史行动分析来增强导航和响应能力，在各种环境条件下实现安全高效的操作。
* **欺诈检测 Agent：** 通过使用新识别的欺诈模式改进预测模型来增强异常检测能力，提高系统安全性并最小化财务损失。
* **推荐 Agent：** 通过采用用户偏好学习算法来提高内容选择精度，提供高度个性化和上下文相关的推荐。
* **游戏 AI Agent：** 通过动态调整战略算法来增强玩家参与度，从而增加游戏复杂性和挑战性。
* **知识库学习 Agent：** Agent 可以利用检索增强生成（RAG）来维护问题描述和已验证解决方案的动态知识库（参见第 14 章）。通过存储成功的策略和遇到的挑战，Agent 可以在决策期间引用这些数据，使其能够通过应用先前成功的模式或避免已知陷阱来更有效地适应新情况。

## 案例研究：自我改进编码 Agent（SICA）

自我改进编码 Agent（SICA）由 Maxime Robeyns、Laurence Aitchison 和 Martin Szummer 开发，代表了基于 Agent 的学习的重要进展，展示了 Agent 修改自身源代码的能力。这与传统方法形成鲜明对比，在传统方法中，一个 Agent 可能训练另一个 Agent；而 SICA 既是修改者又是被修改的实体，通过迭代方式改进其代码库，以提升在各种编码挑战中的性能。

SICA 的自我改进通过迭代循环运行（见图 1）。最初，SICA 审查其过去版本的存档及其在基准测试上的性能表现。它选择具有最高性能分数的版本，该分数基于考虑成功率、时间和计算成本的加权公式计算得出。选定的版本随后进行下一轮自我修改。它分析存档以识别潜在的改进点，然后直接修改其代码库。修改后的 Agent 随后针对基准进行测试，结果记录在存档中。这个过程不断重复，促进从过去性能中直接学习。这种自我改进机制使 SICA 能够在不需要传统训练范式的情况下演化其能力。

![][image1]

图 1：SICA 的自我改进过程，基于其过去版本进行学习和适应

SICA 经历了显著的自我改进，在代码编辑和导航方面取得了重要进展。最初，SICA 使用基本的文件覆盖方法进行代码更改。随后，它开发了能够进行更智能和上下文相关编辑的"智能编辑器"。这进一步演变为"差异增强智能编辑器"，结合差异进行有针对性的修改和基于模式的编辑，以及"快速覆盖工具"以减少处理需求。

SICA 进一步实现了"最小差异输出优化"和"上下文敏感差异最小化"，使用抽象语法树（AST）解析来提高效率。此外，还添加了"智能编辑器输入规范化器"。在导航方面，SICA 独立创建了"AST 符号定位器"，使用代码的结构图（AST）来识别代码库中的定义。后来，开发了"混合符号定位器"，将快速搜索与 AST 检查相结合。这通过"混合符号定位器中的优化 AST 解析"进一步优化，专注于相关代码部分，提高搜索速度（见图 2）。

![][image2]

图 2：跨迭代的性能表现。关键改进用其相应的工具或 Agent 修改进行标注。（由 Maxime Robeyns、Martin Szummer、Laurence Aitchison 提供）

SICA 的架构包括用于基本文件操作、命令执行和算术计算的基础工具包。它包含结果提交机制和专门子 Agent（编码、问题解决和推理）的调用功能。这些子 Agent 负责分解复杂任务并管理 LLM 的上下文长度，特别是在扩展改进周期期间。

异步监督者（另一个 LLM）监控 SICA 的行为，识别潜在问题，如循环或停滞。它与 SICA 进行通信，必要时可以介入以停止执行。监督者接收 SICA 行动的详细报告，包括调用图和消息及工具操作日志，以识别模式和低效率。

SICA 的 LLM 在其上下文窗口（其短期内存）中以结构化方式组织信息，这对操作至关重要。此结构包括定义 Agent 目标的系统提示词、工具和子 Agent 文档以及系统指令。核心提示词包含问题陈述或指令、打开文件的内容和目录映射。助手消息记录 Agent 的逐步推理、工具和子 Agent 调用记录及结果以及监督者通信。这种组织方式促进了高效的信息流动，增强了 LLM 操作并减少了处理时间和成本。最初，文件更改记录为差异，仅显示修改内容并定期合并。

**SICA：代码深度解析：** 深入研究 SICA 的实现揭示了支撑其能力的几个关键设计选择。如前所述，该系统采用模块化架构构建，包含多个子 Agent，如编码 Agent、问题解决 Agent 和推理 Agent。这些子 Agent 由主 Agent 调用，类似于工具调用，用于分解复杂任务并有效管理上下文长度，特别是在这些扩展的元改进迭代期间。

该项目正在积极开发中，旨在为那些对 LLM 在工具使用和其他 Agent 任务上进行后训练感兴趣的人提供一个强大的框架，完整代码可在 [https://github.com/MaximeRobeyns/self_improving_coding_agent/](https://github.com/MaximeRobeyns/self_improving_coding_agent/) GitHub 存储库中供进一步探索和贡献。

出于安全考虑，该项目强烈强调 Docker 容器化，这意味着 Agent 在专用 Docker 容器内运行。这是一个关键措施，因为它提供了与主机的隔离，鉴于 Agent 执行 shell 命令的能力，这减轻了意外文件系统操作等风险。

为确保透明度和控制，系统通过可视化事件总线上的事件和 Agent 调用图的交互式网页提供强大的可观察性。这提供了对 Agent 行动的全面洞察，允许用户检查单个事件、阅读监督者消息并折叠子 Agent 跟踪以获得更清晰的理解。

就其核心智能而言，Agent 框架支持来自各种提供商的 LLM 集成，使用户能够尝试不同的模型以找到特定任务的最佳匹配。最后，一个关键组件是异步监督者，这是一个与主 Agent 并发运行的 LLM。此监督者定期评估 Agent 的行为是否存在病理性偏差或停滞，必要时可以通过发送通知甚至取消 Agent 的执行来介入。它接收系统状态的详细文本表示，包括调用图和 LLM 消息、工具调用和响应的事件流，这使它能够检测低效模式或重复工作。

初始 SICA 实现中的一个显著挑战是提示基于 LLM 的 Agent 在每次元改进迭代期间独立提出新颖、创新、可行且引人入胜的修改。这一限制，特别是在培养 LLM Agent 的开放式学习和真正创造力方面，仍然是当前研究的关键领域。

## AlphaEvolve 和 OpenEvolve

**AlphaEvolve** 是 Google 开发的一个 AI Agent，旨在发现和优化算法。它利用 LLM 的组合，特别是 Gemini 模型（Flash 和 Pro）、自动化评估系统和进化算法框架。该系统旨在推进理论数学和实际计算应用。

AlphaEvolve 采用 Gemini 模型的集合。Flash 用于生成广泛的初始算法提案，而 Pro 提供更深入的分析和改进。然后根据预定义标准自动评估和评分提出的算法。此评估提供用于迭代改进解决方案的反馈，从而产生优化和新颖的算法。

在实际计算中，AlphaEvolve 已部署在 Google 的基础设施中。它在数据中心调度方面展示了改进，导致全球计算资源使用减少 0.7%。它还通过为即将推出的张量处理单元（TPU）的 Verilog 代码提出优化建议来促进硬件设计。此外，AlphaEvolve 加速了 AI 性能，包括 Gemini 架构核心内核的 23% 速度提升以及 FlashAttention 的低级 GPU 指令的最高 32.5% 优化。

在基础研究领域，AlphaEvolve 为矩阵乘法新算法的发现做出了贡献，包括使用 48 次标量乘法的 4x4 复数值矩阵方法，超过了先前已知的解决方案。在更广泛的数学研究中，它在 75% 的情况下重新发现了超过 50 个开放问题的现有最先进解决方案，并在 20% 的情况下改进了现有解决方案，例子包括接吻数问题的进步。

**OpenEvolve** 是一个利用 LLM（见图 3）迭代优化代码的进化编码 Agent。它编排 LLM 驱动的代码生成、评估和选择管道，以持续增强各种任务的程序。OpenEvolve 的一个关键方面是其演化整个代码文件的能力，而不是局限于单个函数。该 Agent 设计具有多功能性，提供对多种编程语言的支持以及与任何 LLM 的 OpenAI 兼容 API 的兼容性。此外，它结合了多目标优化，允许灵活的提示词工程，并能够进行分布式评估以有效处理复杂的编码挑战。

![][image3]

图 3：OpenEvolve 内部架构由控制器管理。该控制器编排几个关键组件：程序采样器、程序数据库、评估器池和 LLM 集合。其主要功能是促进它们的学习和适应过程以提高代码质量。

此代码片段使用 OpenEvolve 库对程序执行进化优化。它使用初始程序、评估文件和配置文件的路径初始化 OpenEvolve 系统。`evolve.run(iterations=1000)` 行启动进化过程，运行 1000 次迭代以找到程序的改进版本。最后，它打印在进化过程中找到的最佳程序的指标，格式化为四位小数。

```python
from openevolve import OpenEvolve

## 初始化系统
evolve = OpenEvolve(
    initial_program_path="path/to/initial_program.py",
    evaluation_file="path/to/evaluator.py",
    config_path="path/to/config.yaml"
)

## 运行进化
best_program = await evolve.run(iterations=1000)

print(f"最佳程序指标：")
for name, value in best_program.metrics.items():
    print(f"  {name}: {value:.4f}")
```

## 概览

**是什么：** AI Agent 通常在动态和不可预测的环境中运行，其中预编程逻辑是不够的。当面对初始设计期间未预料到的新情况时，它们的性能可能会下降。没有从经验中学习的能力，Agent 无法随时间优化其策略或个性化其交互。这种刚性限制了它们的有效性，并阻止它们在复杂的现实世界场景中实现真正的自主性。

**为什么：** 标准化解决方案是集成学习和适应机制，将静态 Agent 转变为动态的、演化的系统。这使 Agent 能够基于新数据和交互自主改进其知识和行为。Agent 系统可以使用各种方法，从强化学习到更高级的技术，如自我改进编码 Agent（SICA）中看到的自我修改。像 Google 的 AlphaEvolve 这样的高级系统利用 LLM 和进化算法来发现全新的、更高效的复杂问题解决方案。通过持续学习，Agent 可以掌握新任务、增强其性能并适应变化的条件，而无需持续的手动重新编程。

**经验法则：** 在构建必须在动态、不确定或演化环境中运行的 Agent 时使用此模式。它对于需要个性化、持续性能改进以及自主处理新情况的能力的应用至关重要。

**视觉摘要**

**![][image4]**

图 4：学习和适应模式

## 关键要点

* 学习和适应是 Agent 通过使用其经验来改进其行为并处理新情况的过程。
* "适应"是来自学习的 Agent 行为或知识的可见变化。
* SICA（自我改进编码 Agent）通过基于过去性能修改其代码来自我改进。这导致了像智能编辑器和 AST 符号定位器这样的工具。
* 拥有专门的"子 Agent"和"监督者"有助于这些自我改进系统管理大任务并保持正轨。
* LLM 的"上下文窗口"的设置方式（包括系统提示词、核心提示词和助手消息）对 Agent 的工作效率至关重要。
* 此模式对于需要在始终变化、不确定或需要个性化交互的环境中运行的 Agent 至关重要。
* 构建学习 Agent 通常意味着将它们与机器学习工具连接并管理数据流。
* 配备基本编码工具的 Agent 系统可以自主编辑自身，从而提高其在基准任务上的性能。
* AlphaEvolve 是 Google 的 AI Agent，利用 LLM 和进化框架自主发现和优化算法，显著增强基础研究和实际计算应用。

## 结论

本章探讨了学习和适应在人工智能中的关键作用。AI Agent 通过持续的数据获取和经验来增强其性能。自我改进编码 Agent（SICA）通过代码修改自主改进其能力，很好地例证了这一点。

我们已经回顾了 Agent AI 的基本组成部分，包括架构、应用、规划、多 Agent 协作、内存管理以及学习和适应。学习原理对于多 Agent 系统中的协调改进特别重要。为了实现这一点，调优数据必须准确反映完整的交互轨迹，捕获每个参与 Agent 的个体输入和输出。

这些元素促成了重大进展，如 Google 的 AlphaEvolve。这个 AI 系统通过 LLM、自动化评估和进化方法独立发现和改进算法，推动科学研究和计算技术的进步。这些模式可以组合起来构建复杂的 AI 系统。像 AlphaEvolve 这样的发展表明，AI Agent 的自主算法发现和优化是可以实现的。

## 参考文献

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
3. Mitchell, T. M. (1997). *Machine Learning*. McGraw-Hill.
4. Proximal Policy Optimization Algorithms by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. You can find it on arXiv: [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
5. Robeyns, M., Aitchison, L., & Szummer, M. (2025). *A Self-Improving Coding Agent*. arXiv:2504.15228v2. [https://arxiv.org/pdf/2504.15228](https://arxiv.org/pdf/2504.15228)  [https://github.com/MaximeRobeyns/self_improving_coding_agent](https://github.com/MaximeRobeyns/self_improving_coding_agent)
6. AlphaEvolve blog, [https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/)
7. OpenEvolve, [https://github.com/codelion/openevolve](https://github.com/codelion/openevolve)

[image1]: ../images/chapter-9/image1.png

[image2]: ../images/chapter-9/image2.png

[image3]: ../images/chapter-9/image3.png

[image4]: ../images/chapter-9/image4.png